{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqH8P65Fk8IP",
        "outputId": "a3467f34-1e8d-4989-ee24-04b6ce1d5d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: eli5 in /Users/divishrengasamy/anaconda3/lib/python3.6/site-packages (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: graphviz in /Users/divishrengasamy/anaconda3/lib/python3.6/site-packages (from eli5) (0.13.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /Users/divishrengasamy/anaconda3/lib/python3.6/site-packages (from eli5) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: attrs>16.0.0 in /Users/divishrengasamy/anaconda3/lib/python3.6/site-packages (from eli5) (19.3.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.18 in /Users/divishrengasamy/anaconda3/lib/python3.6/site-packages (from eli5) (0.22)\n",
            "Requirement already satisfied, skipping upgrade: tabulate>=0.7.7 in /Users/divishrengasamy/anaconda3/lib/python3.6/site-packages (from eli5) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /Users/divishrengasamy/anaconda3/lib/python3.6/site-packages (from eli5) (1.19.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /Users/divishrengasamy/anaconda3/lib/python3.6/site-packages (from eli5) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /Users/divishrengasamy/anaconda3/lib/python3.6/site-packages (from eli5) (2.11.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/divishrengasamy/anaconda3/lib/python3.6/site-packages (from scikit-learn>=0.18->eli5) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /Users/divishrengasamy/anaconda3/lib/python3.6/site-packages (from jinja2->eli5) (1.1.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install eli5 --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChAt_xjRk8IR"
      },
      "outputs": [],
      "source": [
        "from statistics import mean, stdev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueNggghIk8IR"
      },
      "outputs": [],
      "source": [
        "import eli5\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import researchpy as rp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNsTiOBik8IS"
      },
      "outputs": [],
      "source": [
        "from statistics import mean, stdev\n",
        "\n",
        "import eli5\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import researchpy as rp\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from captum.attr import IntegratedGradients, LayerConductance, NeuronConductance\n",
        "from eli5.sklearn import PermutationImportance\n",
        "from scipy.cluster import hierarchy\n",
        "from scipy.stats import mannwhitneyu, spearmanr\n",
        "from sklearn import preprocessing\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.linear_model import SGDRegressor, Ridge, Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import (\n",
        "    StratifiedShuffleSplit,\n",
        "    cross_validate,\n",
        "    train_test_split,\n",
        ")\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.multicomp import MultiComparison, pairwise_tukeyhsd\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "\n",
        "plt.style.use(\"seaborn-pastel\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_DOl83Wk8IS",
        "outputId": "2fd37a2d-ef17-46ab-ea38-e131c023c732"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>pre { white-space: pre !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.core.display import HTML\n",
        "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDt3Ec56k8IT"
      },
      "source": [
        "# Read data and basic preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91Jh-R4kk8IU",
        "outputId": "c5241872-5a14-4609-ee4c-25798a4af053"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1b24938871b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#df_creep = pd.read_csv(\"No anomaly data/complete_creep_no_anomaly_upsampled.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# df_creep = pd.read_csv('No anomaly data/complete_creep_no_anomaly_upsampled_unseen_45m.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_creep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No ABVSM/complete_creep_no_abvsm_upsampled.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_creep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"num_holes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"laser_angle\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"build_orientation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"solidity\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"density\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "#df_creep = pd.read_csv(\"No anomaly data/complete_creep_no_anomaly_upsampled.csv\")\n",
        "# df_creep = pd.read_csv('No anomaly data/complete_creep_no_anomaly_upsampled_unseen_45m.csv')\n",
        "df_creep = pd.read_csv('No ABVSM/complete_creep_no_abvsm_upsampled.csv')\n",
        "df_creep.rename(columns={\"label\": \"num_holes\", \"laser_angle\": \"build_orientation\",\"solidity\":\"density\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stdR8bWxk8IU",
        "outputId": "cd9578a1-95ea-44a1-9234-629fe4abff42"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_creep' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1c3552c3dc31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_creep_disp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_creep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_creep_disp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"creep rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m df_creep_disp.drop(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_creep' is not defined"
          ]
        }
      ],
      "source": [
        "# Display version\n",
        "\n",
        "df_creep_disp = df_creep.copy()\n",
        "df_creep_disp.sort_values(by=[\"creep rate\"], inplace=True)\n",
        "df_creep_disp.drop(\n",
        "    [\"euler_number\", \"extent\", \"conditions\", \"max_intensity\", \"mean_intensity\", \"min_intensity\"], inplace=True, axis=1\n",
        ")\n",
        "df_creep_disp.rename(columns={\"label\": \"num_holes\", \"solidity\":\"density\"}, inplace=True)\n",
        "df_creep_disp_X = df_creep_disp.drop([\"creep rate\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsgZNyxpk8IV"
      },
      "outputs": [],
      "source": [
        "# Corr version\n",
        "\n",
        "df_creep_corr = df_creep.copy()\n",
        "df_creep_corr.sort_values(by=[\"creep rate\"], inplace=True)\n",
        "df_creep_corr.drop(\n",
        "    [\n",
        "        \"euler_number\",\n",
        "        \"extent\",\n",
        "        \"conditions\",\n",
        "        \"max_intensity\",\n",
        "        \"mean_intensity\",\n",
        "        \"min_intensity\",\n",
        "        \"scan_strategy\",\n",
        "        \"laser_num\",\n",
        "        \"build_orientation\",\n",
        "    ],\n",
        "    inplace=True,\n",
        "    axis=1,\n",
        ")\n",
        "df_creep_corr.rename(columns={\"label\": \"num_holes\", \"solidity\":\"density\"}, inplace=True)\n",
        "corr_column_names = np.array(df_creep_corr.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1xHxkYLk8IV"
      },
      "outputs": [],
      "source": [
        "# Anova version\n",
        "df_creep_anova = df_creep.copy()\n",
        "df_creep_anova.sort_values(by=[\"creep rate\"], inplace=True)\n",
        "df_creep_anova.drop(\n",
        "    [\"euler_number\", \"extent\", \"conditions\", \"max_intensity\", \"mean_intensity\", \"min_intensity\"], inplace=True, axis=1\n",
        ")\n",
        "df_creep_anova.rename(columns={\"label\": \"num_holes\", \"solidity\":\"density\"}, inplace=True)\n",
        "df_creep_anova.rename(columns={\"creep rate\": \"creep_rate\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2FbKgb5k8IW"
      },
      "outputs": [],
      "source": [
        "df_creep[\"laser_num\"] = df_creep[\"laser_num\"].astype(\"category\").cat.codes\n",
        "df_creep[\"build_orientation\"] = df_creep[\"build_orientation\"].astype(\"category\").cat.codes\n",
        "df_creep[\"scan_strategy\"] = df_creep[\"scan_strategy\"].astype(\"category\").cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzuBTxDbk8IW"
      },
      "outputs": [],
      "source": [
        "df_creep.drop(\n",
        "    [\"euler_number\", \"extent\", \"conditions\", \"max_intensity\", \"mean_intensity\", \"min_intensity\"], inplace=True, axis=1\n",
        ")\n",
        "df_creep.rename(columns={\"label\": \"num_holes\", \"creep rate\": \"creep_rate\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_G-44Gyk8IW"
      },
      "outputs": [],
      "source": [
        "df_creep.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN2nZGFtk8IW"
      },
      "outputs": [],
      "source": [
        "df_creep_copy = df_creep.copy()\n",
        "df_creep_X = df_creep_copy.drop([\"creep_rate\"], axis=1)\n",
        "df_creep_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx6GSVzAk8IW"
      },
      "outputs": [],
      "source": [
        "column_names = np.array(df_creep.columns)\n",
        "column_names = column_names[0:-1]\n",
        "\n",
        "# Scaling df_creep_X\n",
        "v = df_creep_X.values  # returns a numpy array\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "v_scaled = min_max_scaler.fit_transform(v)\n",
        "df_creep_X = pd.DataFrame(v_scaled, columns=column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do3EGHXQk8IW"
      },
      "outputs": [],
      "source": [
        "df_creep_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oknu8ac4k8IW"
      },
      "outputs": [],
      "source": [
        "creep_data = df_creep.to_numpy()\n",
        "creep_data_cont = creep_data[:, :-4]\n",
        "creep_data_cat = creep_data[:, -4:]\n",
        "\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "creep_data_cont_scaled = min_max_scaler.fit_transform(creep_data_cont)\n",
        "creep_data = np.hstack((creep_data_cont_scaled, creep_data_cat))\n",
        "\n",
        "creep_data_X = creep_data[:, :-1]\n",
        "creep_data_y = creep_data[:, -1]\n",
        "\n",
        "# scaled output\n",
        "creep_data_y = creep_data_y * 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CGkf1cOk8IX"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    creep_data_X, creep_data_y, test_size=0.2, shuffle=True, random_state=42\n",
        ")\n",
        "\n",
        "# vm test size ratio 0.033444816053511704"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDQp57SRk8IX"
      },
      "source": [
        "# Correlation and $R^2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWXMlUINk8IX"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
        "corr = spearmanr(df_creep_corr).correlation\n",
        "corr = np.square(corr)\n",
        "corr_linkage = hierarchy.ward(corr)\n",
        "ax1.set_xlabel('Hierachy level')\n",
        "dendro = hierarchy.dendrogram(corr_linkage, labels=corr_column_names, ax=ax1, leaf_rotation=0,  orientation='right')\n",
        "dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
        "\n",
        "\n",
        "mapping = ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]], cmap='RdBu',)\n",
        "ax2.set_xticks(dendro_idx)\n",
        "ax2.set_yticks(dendro_idx)\n",
        "ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
        "ax2.set_yticklabels(dendro[\"ivl\"])\n",
        "fig.tight_layout()\n",
        "fig.colorbar(mapping, ax=ax2,fraction=0.046, pad=0.04)\n",
        "plt.figure(dpi=1200)\n",
        "fig.savefig('dendo.png', dpi=1200,bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erZnlPpqk8IX"
      },
      "outputs": [],
      "source": [
        "df_creep_anova[\"scan_strategy\"].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRn5bSHYk8IX"
      },
      "source": [
        "# 1-Way ANOVA test for effects of Printing Conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpUDqRE9k8IX"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import HTML, display\n",
        "\n",
        "display(HTML(\"<script>$('div.cell.selected').next().height(100);</script>\"))\n",
        "\n",
        "\n",
        "def one_way_anova(feature):\n",
        "    print(\"*************************************************************************\")\n",
        "    print(\"                            ONE WAY ANOVA TEST\")\n",
        "    print(\"*************************************************************************\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f\"  Summary of {feature} group by scan strategy, laser number, and build orientation.\")\n",
        "    print(\"==============================================================================\")\n",
        "    print(rp.summary_cont(df_creep_anova[f\"{feature}\"].groupby(df_creep_anova[\"scan_strategy\"])))\n",
        "    print(rp.summary_cont(df_creep_anova[f\"{feature}\"].groupby(df_creep_anova[\"laser_num\"])))\n",
        "    print(rp.summary_cont(df_creep_anova[f\"{feature}\"].groupby(df_creep_anova[\"build_orientation\"])))\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"Scan strategy OLS Summary\")\n",
        "    results_scan_strategy = ols(f\"{feature} ~ C(scan_strategy)\", data=df_creep_anova).fit()\n",
        "    print(results_scan_strategy.summary())\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"Number of laser OLS Summary\")\n",
        "    results_laser_num = ols(f\"{feature} ~ C(laser_num)\", data=df_creep_anova).fit()\n",
        "    print(results_laser_num.summary())\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"Building orientation OLS Summary\")\n",
        "    results_build_orientation = ols(f\"{feature} ~ C(build_orientation)\", data=df_creep_anova).fit()\n",
        "    print(results_build_orientation.summary())\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "    print(f\"                               Model Effect Size\")\n",
        "    print(\"==============================================================================\")\n",
        "    aov_table_scan_strategy = sm.stats.anova_lm(results_scan_strategy, typ=2)\n",
        "    aov_table_laser_num = sm.stats.anova_lm(results_laser_num, typ=2)\n",
        "    aov_table_build_orientation = sm.stats.anova_lm(results_build_orientation, typ=2)\n",
        "\n",
        "    def anova_table(aov):\n",
        "        aov[\"mean_sq\"] = aov[:][\"sum_sq\"] / aov[:][\"df\"]\n",
        "\n",
        "        aov[\"eta_sq\"] = aov[:-1][\"sum_sq\"] / sum(aov[\"sum_sq\"])\n",
        "\n",
        "        aov[\"omega_sq\"] = (aov[:-1][\"sum_sq\"] - (aov[:-1][\"df\"] * aov[\"mean_sq\"][-1])) / (\n",
        "            sum(aov[\"sum_sq\"]) + aov[\"mean_sq\"][-1]\n",
        "        )\n",
        "\n",
        "        cols = [\"sum_sq\", \"df\", \"mean_sq\", \"F\", \"PR(>F)\", \"eta_sq\", \"omega_sq\"]\n",
        "        aov = aov[cols]\n",
        "        return aov\n",
        "\n",
        "    print(\"Model Effect Size: Scan strategy\")\n",
        "    display(anova_table(aov_table_scan_strategy))\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"Model Effect Size: Laser number\")\n",
        "    display(anova_table(aov_table_laser_num))\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"Model Effect Size: Build orientation\")\n",
        "    display(anova_table(aov_table_build_orientation))\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f\"                               Post-hoc Testing - difference between groups\")\n",
        "    print(\"==============================================================================\")\n",
        "    print(f\"Post-hoc testing of scan strategy with {feature}\")\n",
        "    mc = MultiComparison(df_creep_anova[f\"{feature}\"], df_creep_anova[\"scan_strategy\"])\n",
        "    mc_results = mc.tukeyhsd()\n",
        "    print(mc_results)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f\"Post-hoc testing of laser number with {feature}\")\n",
        "    mc = MultiComparison(df_creep_anova[f\"{feature}\"], df_creep_anova[\"laser_num\"])\n",
        "    mc_results = mc.tukeyhsd()\n",
        "    print(mc_results)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f\"Post-hoc testing of build orientation with {feature}\")\n",
        "    mc = MultiComparison(df_creep_anova[f\"{feature}\"], df_creep_anova[\"build_orientation\"])\n",
        "    mc_results = mc.tukeyhsd()\n",
        "    print(mc_results)\n",
        "\n",
        "\n",
        "one_way_anova(\"creep_rate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKgMTmyGk8IY"
      },
      "outputs": [],
      "source": [
        "one_way_anova(\"num_holes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZly4iYMk8IY"
      },
      "source": [
        "# 3-Way ANOVA test for effects of Printing Conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Yh78WiQk8IY"
      },
      "outputs": [],
      "source": [
        "def three_way_anova(feature):\n",
        "    print(\"*************************************************************************\")\n",
        "    print(\"                            THREE WAY ANOVA TEST\")\n",
        "    print(\"*************************************************************************\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f\"  Summary of {feature} group by scan strategy, laser number, and build orientation.\")\n",
        "    print(\"==============================================================================\")\n",
        "    display(rp.summary_cont(df_creep_anova.groupby([\"scan_strategy\", \"laser_num\", \"build_orientation\"]))[f\"{feature}\"])\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"Scan strategy, laser number, and build orientation OLS Summary\")\n",
        "    model = ols(f\"{feature} ~ C(scan_strategy)*C(laser_num)*C(build_orientation)\", data=df_creep_anova).fit()\n",
        "    display(model.summary())\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"Anova Table\")\n",
        "    res = sm.stats.anova_lm(model, typ=3)\n",
        "    print(res)\n",
        "    # print(f\"Overall model F({model.df_model: .0f},{model.df_resid: .0f}) = {model.fvalue: .3f}, p = {model.f_pvalue: .4f}\")\n",
        "\n",
        "    print(f\"                               Model Effect Size\")\n",
        "    print(\"==============================================================================\")\n",
        "    #aov_table = sm.stats.anova_lm(res, typ=2)\n",
        "\n",
        "    def anova_table(aov):\n",
        "        aov[\"mean_sq\"] = aov[:][\"sum_sq\"] / aov[:][\"df\"]\n",
        "\n",
        "        aov[\"eta_sq\"] = aov[:-1][\"sum_sq\"] / sum(aov[\"sum_sq\"])\n",
        "\n",
        "        aov[\"omega_sq\"] = (aov[:-1][\"sum_sq\"] - (aov[:-1][\"df\"] * aov[\"mean_sq\"][-1])) / (\n",
        "            sum(aov[\"sum_sq\"]) + aov[\"mean_sq\"][-1]\n",
        "        )\n",
        "\n",
        "        cols = [\"sum_sq\", \"df\", \"mean_sq\", \"F\", \"PR(>F)\", \"eta_sq\", \"omega_sq\"]\n",
        "        aov = aov[cols]\n",
        "        return aov\n",
        "\n",
        "    display(anova_table(res))\n",
        "\n",
        "\n",
        "    print('')\n",
        "    print('')\n",
        "    print('')\n",
        "    print('')\n",
        "    print(f'                               Post-hoc Testing - difference between groups')\n",
        "    print('==============================================================================')\n",
        "    print(f'Post-hoc testing of scan strategy with {feature}')\n",
        "    mc = MultiComparison(df_creep_anova[f'{feature}'], df_creep_anova['scan_strategy'])\n",
        "    mc_results = mc.tukeyhsd()\n",
        "    print(mc_results)\n",
        "    print('')\n",
        "    print('')\n",
        "    print('')\n",
        "    print(f'Post-hoc testing of laser number with {feature}')\n",
        "    mc = MultiComparison(df_creep_anova[f'{feature}'], df_creep_anova['laser_num'])\n",
        "    mc_results = mc.tukeyhsd()\n",
        "    print(mc_results)\n",
        "    print('')\n",
        "    print('')\n",
        "    print('')\n",
        "    print(f'Post-hoc testing of build orientation with {feature}')\n",
        "    mc = MultiComparison(df_creep_anova[f'{feature}'], df_creep_anova['build_orientation'])\n",
        "    mc_results = mc.tukeyhsd()\n",
        "    print(mc_results)\n",
        "\n",
        "\n",
        "three_way_anova(\"creep_rate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jJ0Xjoxk8IY"
      },
      "outputs": [],
      "source": [
        "three_way_anova(\"num_holes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XONBqk5k8IY"
      },
      "source": [
        "## Define Mean  Absolute Percentage Error function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9kA0NTwk8IY"
      },
      "outputs": [],
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return (np.mean(np.abs((y_true - y_pred) / y_true)))\n",
        "\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imYiA-y3k8IY"
      },
      "source": [
        "# Cross Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbKF5D2zk8IY"
      },
      "outputs": [],
      "source": [
        "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SZsLD4yk8IY"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "\n",
        "sklearn.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39zJjlG7k8IY"
      },
      "source": [
        "# Fit Random Forest model and predict creep rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj2FeBLgk8IY"
      },
      "outputs": [],
      "source": [
        "rf_regr = RandomForestRegressor(max_depth=5, n_estimators=500, random_state=0)\n",
        "rf_regr.fit(X_train, y_train)\n",
        "rf_predicted = rf_regr.predict(X_test)\n",
        "\n",
        "\n",
        "scoring = {'r2': 'r2',\n",
        "           'mape':mape_scorer,\n",
        "           'neg_median_absolute_error':'neg_median_absolute_error',\n",
        "           'neg_root_mean_squared_error':'neg_root_mean_squared_error',\n",
        "           'neg_mean_absolute_error':'neg_mean_absolute_error'}\n",
        "\n",
        "rf_scores = cross_validate(\n",
        "    rf_regr,\n",
        "    creep_data_X,\n",
        "    creep_data_y,\n",
        "    cv=sss,\n",
        "    #scoring=(\"r2\", \"neg_median_absolute_error\", \"neg_root_mean_squared_error\", \"neg_mean_absolute_error\"),\n",
        "    scoring = scoring\n",
        ")\n",
        "\n",
        "print(\n",
        "    f'The median absolute error using random forest is: {(rf_scores[\"test_neg_median_absolute_error\"].mean())*-1:.4f}\\u00B1{(rf_scores[\"test_neg_median_absolute_error\"].std()):.4f}'\n",
        ")\n",
        "print(\n",
        "    f'The R2 using random forest is: {(rf_scores[\"test_r2\"].mean()):.4f}\\u00B1{(rf_scores[\"test_r2\"].std()):.4f}'\n",
        ")\n",
        "print(\n",
        "    f'The mean absolute error using random forest is: {(rf_scores[\"test_neg_mean_absolute_error\"].mean())*-1:.4f}\\u00B1{(rf_scores[\"test_neg_mean_absolute_error\"].std()):.4f}'\n",
        ")\n",
        "print(\n",
        "    f'The mean absolute percentage error using random forest is {(rf_scores[\"test_mape\"].mean())*-1:.4f}\\u00B1{(rf_scores[\"test_mape\"].std()):.4f}'\n",
        ")\n",
        "print(\n",
        "    f'The RMSE using random forest is: {(rf_scores[\"test_neg_root_mean_squared_error\"].mean())*-1:.4f}\\u00B1{(rf_scores[\"test_neg_root_mean_squared_error\"].std()):.4f}'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj18bZTKk8IZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(y_test, label=\"True\")\n",
        "plt.plot(rf_predicted, label=\"Predicted\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"Random Forest prediction of creep rate\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfiv1sNMk8IZ"
      },
      "outputs": [],
      "source": [
        "rf_feature_imp = rf_regr.feature_importances_\n",
        "pos = np.arange(column_names.size)\n",
        "\n",
        "combined_rf_imp_name = np.stack((rf_feature_imp, column_names), axis=-1)\n",
        "sorted_combined_rf_imp_name = combined_rf_imp_name[combined_rf_imp_name[:, 0].argsort()]\n",
        "\n",
        "sorted_rf_feature_imp = sorted_combined_rf_imp_name[:, 0]\n",
        "sorted_column_names = sorted_combined_rf_imp_name[:, 1]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.bar(pos, sorted_rf_feature_imp, align=\"center\", alpha=0.5)\n",
        "plt.xticks(pos, sorted_column_names)\n",
        "plt.title(\"RF Default Feature using Mean Decrease in Impurity Importances (test set)\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL0sJZNuk8IZ"
      },
      "outputs": [],
      "source": [
        "# scoring\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aww0Fycrk8IZ"
      },
      "outputs": [],
      "source": [
        "#pi_result_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS1d6Ux_k8IZ"
      },
      "outputs": [],
      "source": [
        "pi_result_rf = permutation_importance(rf_regr, X_test, y_test, n_repeats=10, random_state=42, scoring=mae_scorer)\n",
        "# sorted_idx = pi_result_rf.importances_mean.argsort()\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "# plt.bar(pos,pi_result_rf.importances_mean[sorted_idx].T,align='center', alpha=0.5)\n",
        "# plt.xticks(pos, column_names[sorted_idx])\n",
        "plt.bar(pos, pi_result_rf.importances_mean, align=\"center\", alpha=0.5)\n",
        "plt.xticks(pos, column_names)\n",
        "plt.title(\"RF Permutation Importances (test set)\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iXP9UG4k8IZ"
      },
      "outputs": [],
      "source": [
        "sorted_idx = pi_result_rf.importances_mean.argsort()\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.boxplot(pi_result_rf.importances[sorted_idx].T, vert=True, labels=column_names[sorted_idx])\n",
        "plt.title(\"RF Permutation Importances (test set) with mean and std\")\n",
        "# fig.tight_layout()\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMI-9P3Gk8IZ"
      },
      "source": [
        "# Fit Gradient Boosting model and predict creep rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TZdyVMTk8IZ"
      },
      "outputs": [],
      "source": [
        "gb_regr = GradientBoostingRegressor(max_depth=5, n_estimators=500, random_state=0)\n",
        "gb_regr.fit(X_train, y_train)\n",
        "gb_predicted = gb_regr.predict(X_test)\n",
        "\n",
        "\n",
        "scoring = {'r2': 'r2',\n",
        "           'mape':mape_scorer,\n",
        "           'neg_median_absolute_error':'neg_median_absolute_error',\n",
        "           'neg_root_mean_squared_error':'neg_root_mean_squared_error',\n",
        "           'neg_mean_absolute_error':'neg_mean_absolute_error'}\n",
        "\n",
        "gb_scores = cross_validate(\n",
        "    gb_regr,\n",
        "    creep_data_X,\n",
        "    creep_data_y,\n",
        "    cv=sss,\n",
        "    #scoring=(\"r2\", \"neg_median_absolute_error\", \"neg_root_mean_squared_error\", \"neg_mean_absolute_error\"),\n",
        "    scoring=scoring\n",
        ")\n",
        "\n",
        "print(\n",
        "    f'The median absolute error using Gradient Boosting is: {(gb_scores[\"test_neg_median_absolute_error\"].mean())*-1:.4f}\\u00B1{(gb_scores[\"test_neg_median_absolute_error\"].std()):.4f}'\n",
        ")\n",
        "print(f'The R2 using Gradient Boosting is: {(gb_scores[\"test_r2\"].mean()):.4f}\\u00B1{(gb_scores[\"test_r2\"].std()):.4f}')\n",
        "print(\n",
        "    f'The mean absolute error using Gradient Boosting is: {(gb_scores[\"test_neg_mean_absolute_error\"].mean())*-1:.4f}\\u00B1{(gb_scores[\"test_neg_mean_absolute_error\"].std()):.4f}'\n",
        ")\n",
        "print(\n",
        "    f'The mean absolute percentage error using random forest is {(gb_scores[\"test_mape\"].mean())*-1:.4f}\\u00B1{(gb_scores[\"test_mape\"].std()):.4f}'\n",
        ")\n",
        "print(\n",
        "    f'The RMSE using Gradient Boosting is: {(gb_scores[\"test_neg_root_mean_squared_error\"].mean())*-1:.4f}\\u00B1{(gb_scores[\"test_neg_root_mean_squared_error\"].std()):.4f}'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blaNn1_qk8IZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(y_test, label=\"True\")\n",
        "plt.plot(gb_predicted, label=\"Predicted\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"Gradient Boosting prediction of creep rate\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vp8fvTCk8IZ"
      },
      "outputs": [],
      "source": [
        "gb_feature_imp = gb_regr.feature_importances_\n",
        "pos = np.arange(column_names.size)\n",
        "\n",
        "combined_gb_imp_name = np.stack((gb_feature_imp, column_names), axis=-1)\n",
        "sorted_combined_gb_imp_name = combined_gb_imp_name[combined_gb_imp_name[:, 0].argsort()]\n",
        "\n",
        "sorted_gb_feature_imp = sorted_combined_gb_imp_name[:, 0]\n",
        "sorted_column_names = sorted_combined_gb_imp_name[:, 1]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.bar(pos, sorted_gb_feature_imp, align=\"center\", alpha=0.5, label=\"Gradient Boosting\")\n",
        "plt.xticks(pos, sorted_column_names)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"GB Default Feature using Mean Decrease in Impurity Importances (test set)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lZCoEZwk8IZ"
      },
      "outputs": [],
      "source": [
        "pi_result_gb = permutation_importance(gb_regr, X_test, y_test, n_repeats=10, random_state=42, scoring=mae_scorer)\n",
        "\n",
        "sorted_idx = pi_result_gb.importances_mean.argsort()\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "# plt.bar(pos,pi_result_gb.importances_mean[sorted_idx].T,align='center', alpha=0.5)\n",
        "# plt.xticks(pos, column_names[sorted_idx])\n",
        "plt.bar(pos, pi_result_gb.importances_mean, align=\"center\", alpha=0.5)\n",
        "plt.xticks(pos, column_names)\n",
        "plt.title(\"GB Permutation Importances (test set)\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYBUV50Uk8IZ"
      },
      "outputs": [],
      "source": [
        "sorted_idx = pi_result_gb.importances_mean.argsort()\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.boxplot(pi_result_gb.importances[sorted_idx].T, vert=True, labels=column_names[sorted_idx])\n",
        "plt.title(\"GB Permutation Importances (test set) with mean and std\")\n",
        "# fig.tight_layout()\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckWIadg3k8IZ"
      },
      "source": [
        "## Fit Deep Learning predict creep rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cctxPxMk8IZ"
      },
      "outputs": [],
      "source": [
        "def base_model():\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(\n",
        "                128,\n",
        "                activation=\"relu\",\n",
        "                input_shape=(creep_data_X.shape[1],),\n",
        "                bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "            ),\n",
        "            layers.Dense(256, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "            layers.Dense(128, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "            layers.Dense(64, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "            layers.Dense(64, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "            layers.Dense(32, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "            layers.Dense(16, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "            layers.Dense(8, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "            layers.Dense(6, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "            layers.Dense(4, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "            layers.Dense(1, activation=\"relu\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    radam = tfa.optimizers.RectifiedAdam()\n",
        "    ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n",
        "    # tf.keras.optimizers.Adam(1e-4)\n",
        "    # Configure a model for mean-squared error regression.\n",
        "    model.compile(optimizer=ranger, loss=\"mse\", metrics=[\"mae\"])  # mean squared error  # mean absolute error\n",
        "    return model\n",
        "\n",
        "\n",
        "# reshape for deep model\n",
        "y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
        "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", min_delta=0, patience=100, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True\n",
        ")\n",
        "\n",
        "# dnn_model = KerasRegressor(\n",
        "#     build_fn=base_model, epochs=1000, batch_size=32, verbose=1, validation_split=0.1, callbacks=[callback]\n",
        "# )\n",
        "\n",
        "# scoring = {'r2': 'r2',\n",
        "#            'mape':mape_scorer,\n",
        "#            'neg_median_absolute_error':'neg_median_absolute_error',\n",
        "#            'neg_root_mean_squared_error':'neg_root_mean_squared_error',\n",
        "#            'neg_mean_absolute_error':'neg_mean_absolute_error'}\n",
        "\n",
        "dnn_cv_model = KerasRegressor(build_fn=base_model,\n",
        "                          epochs=1000,\n",
        "                          batch_size=32,\n",
        "                          verbose=0)\n",
        "\n",
        "dnn_scores = cross_validate(dnn_cv_model,\n",
        "                        creep_data_X,\n",
        "                        creep_data_y,\n",
        "                        cv=sss,\n",
        "#                         scoring=('r2',\n",
        "#                                  'neg_median_absolute_error',\n",
        "#                                  'neg_root_mean_squared_error',\n",
        "#                                  'neg_mean_absolute_error'))\n",
        "                        scoring=mape_scorer)\n",
        "\n",
        "#history = dnn_scores.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUZqIZADk8Ia"
      },
      "outputs": [],
      "source": [
        "dnn_scores['test_score'].mean()*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ2GUveIk8Ia"
      },
      "outputs": [],
      "source": [
        "# no anomaly\n",
        "test_r2 = np.array(\n",
        "    [\n",
        "        0.85180666,\n",
        "        0.99031263,\n",
        "        0.77517249,\n",
        "        0.87576557,\n",
        "        0.67822943,\n",
        "        0.99971448,\n",
        "        0.99996456,\n",
        "        0.99994025,\n",
        "        0.673038,\n",
        "        0.84580081,\n",
        "        0.98899901,\n",
        "        0.92063382,\n",
        "        0.93035812,\n",
        "        0.99959477,\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVHlh5cjk8Ia"
      },
      "outputs": [],
      "source": [
        "# no anomaly\n",
        "test_median_absolute_error = np.array(\n",
        "    [\n",
        "        -0.09521637,\n",
        "        -0.60142975,\n",
        "        -2.42244644,\n",
        "        -0.4693655,\n",
        "        -0.32491474,\n",
        "        -0.07423897,\n",
        "        -0.08073444,\n",
        "        -0.06702423,\n",
        "        -0.25761871,\n",
        "        -0.19247475,\n",
        "        -0.44676132,\n",
        "        -0.58699913,\n",
        "        -0.48970985,\n",
        "        -0.1156929,\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmCaPJWuk8Ia"
      },
      "outputs": [],
      "source": [
        "# no anomaly\n",
        "test_root_mean_squared_error = np.array(\n",
        "    [\n",
        "        -1.70804828,\n",
        "        -1.11181973,\n",
        "        -6.65588786,\n",
        "        -3.20204712,\n",
        "        -3.67897535,\n",
        "        -0.54152522,\n",
        "        -0.19077301,\n",
        "        -0.24773261,\n",
        "        -18.32517945,\n",
        "        -12.58463565,\n",
        "        -3.36136281,\n",
        "        -9.02853612,\n",
        "        -8.45736031,\n",
        "        -0.64513537,\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BwT3UyUk8Ia"
      },
      "outputs": [],
      "source": [
        "test_mean_absolute_error = np.array(\n",
        "    [\n",
        "        -1.70804828,\n",
        "        -1.11181973,\n",
        "        -6.65588786,\n",
        "        -3.20204712,\n",
        "        -3.67897535,\n",
        "        -0.22006422,\n",
        "        -0.13449019,\n",
        "        -0.13811937,\n",
        "        -4.57801037,\n",
        "        -1.91723722,\n",
        "        -1.29813273,\n",
        "        -2.26022571,\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfbIbRLEk8Ia"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMLgou1sk8Ia"
      },
      "outputs": [],
      "source": [
        "# dnn_predicted = dnn_model.predict(X_test)\n",
        "\n",
        "# dnn_predicted = dnn_predicted.reshape(dnn_predicted.shape[0],1)\n",
        "\n",
        "print(\n",
        "    f\"The median absolute error using DNN is: {(test_median_absolute_error.mean())*-1:.4f}\\u00B1{(test_median_absolute_error.std()):.4f}\"\n",
        ")\n",
        "print(f\"The R2 using DNN is: {(test_r2.mean()):.4f}\\u00B1{(test_r2.std()):.4f}\")\n",
        "print(\n",
        "    f\"The mean absolute error using DNN is: {(test_mean_absolute_error.mean())*-1:.4f}\\u00B1{(test_mean_absolute_error.std()):.4f}\"\n",
        ")\n",
        "print(\n",
        "    f\"The RMSE using DNN is: {(test_root_mean_squared_error.mean())*-1:.4f}\\u00B1{(test_root_mean_squared_error.std()):.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlsBxEYAk8Ia"
      },
      "outputs": [],
      "source": [
        "pi_result_gb.importances_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRRW0xnyk8Ia"
      },
      "outputs": [],
      "source": [
        "pi_result_rf.importances_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sS2Yddck8Ia"
      },
      "outputs": [],
      "source": [
        "# TODO FIX SORTING\n",
        "dnn_pi = np.array(\n",
        "    [\n",
        "        4.616778,\n",
        "        1.381472,\n",
        "        1.872345,\n",
        "        3.972396,\n",
        "        4.374675,\n",
        "        1.502202,\n",
        "        4.163034,\n",
        "        2.016819,\n",
        "        1.435615,\n",
        "        4.739331,\n",
        "        1.283143,\n",
        "        0.366622,\n",
        "        1.242507,\n",
        "        0.215502,\n",
        "        0.242231,\n",
        "        4.900941,\n",
        "        7.683940,\n",
        "        11.364592,\n",
        "        17.583885,\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PGFrrDsk8Ia"
      },
      "outputs": [],
      "source": [
        "def normalise(input_):\n",
        "    input_max = input_.max()\n",
        "    input_min = input_.min()\n",
        "    return((input_ - input_min) / (input_max - input_min))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Pigeu-k8Ia"
      },
      "outputs": [],
      "source": [
        "gb_pi = np.abs(pi_result_gb.importances_mean)\n",
        "gb_pi_min = np.abs(pi_result_gb.importances_mean).min()\n",
        "gb_pi_max = np.abs(pi_result_gb.importances_mean).max()\n",
        "\n",
        "gb_pi_scaled = (gb_pi - gb_pi_min) / (gb_pi_max - gb_pi_min)\n",
        "\n",
        "\n",
        "rf_pi = np.abs(pi_result_rf.importances_mean)\n",
        "rf_pi_min = np.abs(pi_result_rf.importances_mean).min()\n",
        "rf_pi_max = np.abs(pi_result_rf.importances_mean).max()\n",
        "\n",
        "rf_pi_scaled = (rf_pi - rf_pi_min) / (rf_pi_max - rf_pi_min)\n",
        "\n",
        "\n",
        "dnn_pi = np.abs(dnn_pi)\n",
        "dnn_pi_min = np.abs(dnn_pi).min()\n",
        "dnn_pi_max = np.abs(dnn_pi).max()\n",
        "\n",
        "dnn_pi_scaled = (dnn_pi - dnn_pi_min) / (dnn_pi_max - dnn_pi_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHDtgwj0k8Ib"
      },
      "outputs": [],
      "source": [
        "(gb_pi_scaled + rf_pi_scaled + dnn_pi_scaled) / 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEw7vysMk8Ib"
      },
      "outputs": [],
      "source": [
        "perm = PermutationImportance(dnn_model, scoring=mae_scorer, random_state=1).fit(X_train, y_train)\n",
        "dnn_pi_df = eli5.explain_weights_df(perm, feature_names=column_names.tolist())\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.bar(pos, dnn_pi_df.weight.values[::-1], align=\"center\", alpha=0.5)\n",
        "plt.xticks(pos, dnn_pi_df.feature.values[::-1])\n",
        "plt.title(\"DNN Permutation Importances (test set)\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzYkUDCPk8Ib"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(pos, ((gb_pi_scaled + rf_pi_scaled + dnn_pi_scaled) / 3), align=\"center\", alpha=0.5)\n",
        "plt.xticks(pos, column_names)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Ensemble average of permutation importance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi3PcK97k8Ib"
      },
      "outputs": [],
      "source": [
        "def combine_mean_std_np(mean_array, std_array):\n",
        "    return f\"{mean_array:.4f}\\u00B1{std_array:.4f}\"\n",
        "\n",
        "\n",
        "idx = np.array([\"MedAE\", \"R2\", \"MAPE\", \"MAE\", \"RMSE\"])\n",
        "col = np.array([\"Random Forest\", \"Gradient Boosting\", \"Deep Neural Network\"])\n",
        "all_medae = [\n",
        "    combine_mean_std_np(\n",
        "        rf_scores[\"test_neg_median_absolute_error\"].mean() * -1, rf_scores[\"test_neg_median_absolute_error\"].std()\n",
        "    ),\n",
        "    combine_mean_std_np(\n",
        "        gb_scores[\"test_neg_median_absolute_error\"].mean() * -1, gb_scores[\"test_neg_median_absolute_error\"].std()\n",
        "    ),\n",
        "    combine_mean_std_np(test_median_absolute_error.mean() * -1, test_median_absolute_error.std()),\n",
        "]\n",
        "\n",
        "all_r2 = [\n",
        "    combine_mean_std_np(rf_scores[\"test_r2\"].mean(), rf_scores[\"test_r2\"].std()),\n",
        "    combine_mean_std_np(gb_scores[\"test_r2\"].mean(), gb_scores[\"test_r2\"].std()),\n",
        "    combine_mean_std_np(test_r2.mean(), test_r2.std()),\n",
        "]\n",
        "\n",
        "all_mae = [\n",
        "    combine_mean_std_np(\n",
        "        rf_scores[\"test_neg_mean_absolute_error\"].mean() * -1, rf_scores[\"test_neg_mean_absolute_error\"].std()\n",
        "    ),\n",
        "    combine_mean_std_np(\n",
        "        gb_scores[\"test_neg_mean_absolute_error\"].mean() * -1, gb_scores[\"test_neg_mean_absolute_error\"].std()\n",
        "    ),\n",
        "    combine_mean_std_np(test_mean_absolute_error.mean() * -1, test_mean_absolute_error.std()),\n",
        "]\n",
        "\n",
        "all_rmse = [\n",
        "    combine_mean_std_np(\n",
        "        rf_scores[\"test_neg_root_mean_squared_error\"].mean() * -1, rf_scores[\"test_neg_root_mean_squared_error\"].std()\n",
        "    ),\n",
        "    combine_mean_std_np(\n",
        "        gb_scores[\"test_neg_root_mean_squared_error\"].mean() * -1, gb_scores[\"test_neg_root_mean_squared_error\"].std()\n",
        "    ),\n",
        "    combine_mean_std_np(test_root_mean_squared_error.mean() * -1, test_root_mean_squared_error.std()),\n",
        "]\n",
        "\n",
        "# mean_absolute_percentage_error\n",
        "\n",
        "mixed_cond_result = pd.DataFrame(np.array([all_medae, all_r2, all_mae, all_rmse]), columns=col, index=idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAy8buyWk8Ib"
      },
      "outputs": [],
      "source": [
        "mixed_cond_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofMy6qAFk8Ib"
      },
      "outputs": [],
      "source": [
        "def is_it_sig(arr1, arr2):\n",
        "    s, p = mannwhitneyu(arr1, arr2)\n",
        "    if p < 0.05:\n",
        "        sig = \"Yes, Significantly different\"\n",
        "    else:\n",
        "        sig = \"No, not significantly different\"\n",
        "    return sig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp_H78ukk8Ib"
      },
      "outputs": [],
      "source": [
        "rf_MedAE = np.random.normal(0.0673, 0.0371, 20)\n",
        "gb_MedAE = np.random.normal(0.0004, 0.0000, 20)\n",
        "dnn_MedAE = np.random.normal(0.4446, 0.5803, 20)\n",
        "\n",
        "rf_R2 = np.random.normal(0.8813, 0.0609, 20)\n",
        "gb_R2 = np.random.normal(0.9400, 0.0946, 20)\n",
        "dnn_R2 = np.random.normal(0.8950, 0.1131, 20)\n",
        "\n",
        "rf_MAE = np.random.normal(3.3117, 0.7132, 20)\n",
        "gb_MAE = np.random.normal(0.9249, 1.1359, 20)\n",
        "dnn_MAE = np.random.normal(2.2419, 1.9027, 20)\n",
        "\n",
        "rf_rmse = np.random.normal(10.6853, 2.7746, 20)\n",
        "gb_rmse = np.random.normal(5.0001, 6.0512, 20)\n",
        "dnn_rmse = np.random.normal(4.9814, 5.2385, 20)\n",
        "\n",
        "print(f\"(MedAe) Are the result between random forest and gradient boosting significant? {is_it_sig(rf_MedAE,gb_MedAE)}\")\n",
        "print(\n",
        "    f\"(MedAe) Are the result between random forest and deep neural network significant? {is_it_sig(rf_MedAE,dnn_MedAE)}\"\n",
        ")\n",
        "print(\n",
        "    f\"(MedAe) Are the result between gradient boosting and deep neural network significant? {is_it_sig(gb_MedAE,dnn_MedAE)}\"\n",
        ")\n",
        "print(f\"\")\n",
        "print(f\"(R2) Are the result between random forest and gradient boosting significant? {is_it_sig(rf_R2,gb_R2)}\")\n",
        "print(f\"(R2) Are the result between random forest and deep neural network significant? {is_it_sig(rf_R2,dnn_R2)}\")\n",
        "print(f\"(R2) Are the result between gradient boosting and deep neural network significant? {is_it_sig(gb_R2,dnn_R2)}\")\n",
        "print(f\"\")\n",
        "print(f\"(MAE) Are the result between random forest and gradient boosting significant? {is_it_sig(rf_MAE,gb_MAE)}\")\n",
        "print(f\"(MAE) Are the result between random forest and deep neural network significant? {is_it_sig(rf_MAE,dnn_MAE)}\")\n",
        "print(\n",
        "    f\"(MAE) Are the result between gradient boosting and deep neural network significant? {is_it_sig(gb_MAE,dnn_MAE)}\"\n",
        ")\n",
        "print(f\"\")\n",
        "print(f\"(RMSE) Are the result between random forest and gradient boosting significant? {is_it_sig(rf_rmse,gb_rmse)}\")\n",
        "print(f\"(RMSE) Are the result between random forest and deep neural network significant? {is_it_sig(rf_rmse,dnn_rmse)}\")\n",
        "print(\n",
        "    f\"(RMSE) Are the result between gradient boosting and deep neural network significant? {is_it_sig(gb_rmse,dnn_rmse)}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la7bRZBxk8Ib"
      },
      "source": [
        "# SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj_GsYNok8Ib"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "shap.initjs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT1LRtmvk8Ib"
      },
      "outputs": [],
      "source": [
        "shap_values = shap.TreeExplainer(gb_regr).shap_values(df_creep_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhhhjuxxk8Ib"
      },
      "source": [
        "#### SHAP for DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_biZ0PYk8Ib"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(\n",
        "            128,\n",
        "            activation=\"relu\",\n",
        "            input_shape=(creep_data_X.shape[1],),\n",
        "            bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "        ),\n",
        "        layers.Dense(256, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        layers.Dense(128, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        layers.Dense(64, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        layers.Dense(64, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        layers.Dense(32, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        layers.Dense(16, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        layers.Dense(8, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        layers.Dense(6, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        layers.Dense(4, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        layers.Dense(1, activation=\"relu\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "radam = tfa.optimizers.RectifiedAdam()\n",
        "ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n",
        "# tf.keras.optimizers.Adam(1e-4)\n",
        "# Configure a model for mean-squared error regression.\n",
        "model.compile(optimizer=ranger, loss=\"mse\", metrics=[\"mae\"])  # mean squared error  # mean absolute error\n",
        "\n",
        "\n",
        "# reshape for deep model\n",
        "y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
        "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", min_delta=0, patience=100, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1)\n",
        "#               validation_split = 0.1,\n",
        "#               callbacks = [callback]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMkDo5g3k8Ib"
      },
      "outputs": [],
      "source": [
        "df_creep_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN6Sm4mgk8Ib"
      },
      "outputs": [],
      "source": [
        "np.mean(np.square((model.predict(X_test)) - y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVx2eRKAk8Ib"
      },
      "outputs": [],
      "source": [
        "explainer_dnn = shap.KernelExplainer(model.predict, data=df_creep_X.iloc[:300, :])\n",
        "shap_values = explainer_dnn.shap_values(df_creep_X.iloc[:300, :], nsamples=200)\n",
        "# #shap_values = explainer_dnn.shap_values(X_test, nsamples=100, link=\"identity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ye9Rc7qk8Ib"
      },
      "outputs": [],
      "source": [
        "shap_values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0zss5pRk8Ib"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values[0], df_creep_X.iloc[:300, :], sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3xZ8Et3k8Ic"
      },
      "outputs": [],
      "source": [
        "df_creep_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-EJ_DiTk8Ic"
      },
      "outputs": [],
      "source": [
        "df_creep[df_creep[\"scan_strategy\"] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEDxkKUYk8Ic"
      },
      "outputs": [],
      "source": [
        "df_creep_disp_X.loc[[45]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhiTWXR-k8Ic"
      },
      "outputs": [],
      "source": [
        "dnn_sv = np.sum(np.mean(np.abs(shap_values), axis=1), axis=0)\n",
        "dnn_sv_min = np.sum(np.mean(np.abs(shap_values), axis=1), axis=0).min()\n",
        "dnn_sv_max = np.sum(np.mean(np.abs(shap_values), axis=1), axis=0).max()\n",
        "\n",
        "dnn_sv_scaled = (dnn_sv - dnn_sv_min) / (dnn_sv_max - dnn_sv_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM7oDQSek8Ic"
      },
      "outputs": [],
      "source": [
        "dnn_sv_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjIp4odSk8Ic"
      },
      "outputs": [],
      "source": [
        "explainer_rf = shap.TreeExplainer(rf_regr)\n",
        "shap_values_rf = explainer_rf.shap_values(df_creep_X)\n",
        "#np.abs(shap_values_rf).mean(0)\n",
        "\n",
        "explainer_gb = shap.TreeExplainer(gb_regr)\n",
        "shap_values_gb = explainer_gb.shap_values(df_creep_X)\n",
        "#np.abs(shap_values_gb).mean(0)\n",
        "\n",
        "# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\n",
        "shap.force_plot(explainer_gb.expected_value, shap_values_gb[0, :], df_creep_X.iloc[0, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMScsNT1k8Ic"
      },
      "outputs": [],
      "source": [
        "explainer_gb = shap.TreeExplainer(gb_regr)\n",
        "shap_values_gb = explainer_gb.shap_values(df_creep_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuZU0ix9k8Ic"
      },
      "outputs": [],
      "source": [
        "np.abs(shap_values_gb).mean(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCmrvNVQk8Ic"
      },
      "source": [
        "### Summary Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5ITpFNvk8Ic"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values_gb, df_creep_X, sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAQPLDhGk8Ic"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values_gb, df_creep_X, plot_type=\"bar\", sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBkLBnz_k8Ic"
      },
      "outputs": [],
      "source": [
        "gb_sv = np.abs(shap_values_gb).mean(0)\n",
        "gb_sv_min = np.abs(shap_values_gb).mean(0).min()\n",
        "gb_sv_max = np.abs(shap_values_gb).mean(0).max()\n",
        "\n",
        "gb_sv_scaled = (gb_sv - gb_sv_min) / (gb_sv_max - gb_sv_min)\n",
        "\n",
        "\n",
        "rf_sv = np.abs(shap_values_rf).mean(0)\n",
        "rf_sv_min = np.abs(shap_values_rf).mean(0).min()\n",
        "rf_sv_max = np.abs(shap_values_rf).mean(0).max()\n",
        "\n",
        "rf_sv_scaled = (rf_sv - rf_sv_min) / (rf_sv_max - rf_sv_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSz6t8vFk8Ic"
      },
      "outputs": [],
      "source": [
        "pos = np.arange(column_names.size)\n",
        "(gb_sv_scaled + dnn_sv_scaled + rf_sv_scaled) / 3\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(pos, (gb_sv_scaled + dnn_sv_scaled + rf_sv_scaled) / 3, align=\"center\", alpha=0.5, label=\"SHAP\")\n",
        "plt.bar(pos, ((gb_pi_scaled + rf_pi_scaled + dnn_pi_scaled) / 3), align=\"center\", alpha=0.5, label=\"PI\")\n",
        "plt.xticks(pos, column_names)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"Ensemble average of SHAP vs Permutation importance\")\n",
        "\n",
        "\n",
        "# pos = np.arange(column_names.size)\n",
        "\n",
        "# combined_gb_imp_name = np.stack((gb_feature_imp, column_names), axis=-1)\n",
        "# sorted_combined_gb_imp_name = combined_gb_imp_name[combined_gb_imp_name[:,0].argsort()]\n",
        "\n",
        "# sorted_gb_feature_imp = sorted_combined_gb_imp_name[:,0]\n",
        "# sorted_column_names = sorted_combined_gb_imp_name[:,1]\n",
        "\n",
        "# plt.figure(figsize=(15,10))\n",
        "# plt.bar(pos, sorted_gb_feature_imp, align='center', alpha=0.5, label='Gradient Boosting')\n",
        "# #plt.bar(pos, rf_feature_imp, align='center', alpha=0.5, label='Random Forest')\n",
        "# plt.xticks(pos, sorted_column_names)\n",
        "# plt.xticks(rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAjdV4J_k8Ic"
      },
      "source": [
        "### Dependence plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB0WGEPhk8Ic"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"num_holes\", shap_values_gb, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJpDDtBuk8Ic"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"laser_angle\", shap_values_gb, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOvlkXbOk8Ic"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"solidity\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci1f_ytdk8Ic"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"scan_strategy\", shap_values_gb, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFn5s6yvk8Ic"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"equivalent_diameter\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKNl_Uiqk8Ic"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"laser_num\", shap_values_gb, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWx-9OD_k8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"eccentricity\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoM8LpVek8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"orientation\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdEN4yhok8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"inertia_tensor-0-0\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJCnEWrKk8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"inertia_tensor-0-1\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDxNq0DNk8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"inertia_tensor-1-0\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXUUhsoBk8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"inertia_tensor-1-1\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvhJLqEpk8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"inertia_tensor_eigvals-1\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpjtSiT6k8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"inertia_tensor_eigvals-0\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90Xdrq1rk8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"convex_area\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMcotfkFk8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"area\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b2GALD_k8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"major_axis_length\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHYhzunHk8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"minor_axis_length\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPHFypnak8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\"perimeter\", shap_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztm4Hz1Fk8Id"
      },
      "source": [
        "### Interaction plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkCcBL_Ak8Id"
      },
      "source": [
        "#### Interaction Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bem_5EmJk8Id"
      },
      "outputs": [],
      "source": [
        "shap_interaction_values = shap.TreeExplainer(gb_regr).shap_interaction_values(df_creep_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIYX6Bb7k8Id"
      },
      "outputs": [],
      "source": [
        "# Interaction summary plot\n",
        "shap.summary_plot(shap_interaction_values, df_creep_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCyri7Ssk8Id"
      },
      "source": [
        "#### Interaction dependance plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UEv_nBEk8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot(\n",
        "#     (\"laser_angle\", \"num_holes\"), shap_interaction_values, df_creep_X, display_features=df_creep_disp_X\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMlxxlR7k8Id"
      },
      "outputs": [],
      "source": [
        "# shap.dependence_plot((\"laser_angle\", \"area\"), shap_interaction_values, df_creep_X, display_features=df_creep_disp_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvWjL4FEk8Id"
      },
      "outputs": [],
      "source": [
        "# # shap.dependence_plot(\n",
        "#     (\"laser_angle\", \"convex_area\"), shap_interaction_values, df_creep_X, display_features=df_creep_disp_X\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGOdCyZBk8Id"
      },
      "outputs": [],
      "source": [
        "# # shap.dependence_plot(\n",
        "#     (\"laser_angle\", \"eccentricity\"), shap_interaction_values, df_creep_X, display_features=df_creep_disp_X\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw79hptsk8Id"
      },
      "source": [
        "# Intergrated Gradient for Deep Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOrClRRkk8Ie"
      },
      "outputs": [],
      "source": [
        "class SimpleNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(19, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.linear2 = nn.Linear(128, 256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.linear3 = nn.Linear(256, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.linear4 = nn.Linear(128, 64)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.linear5 = nn.Linear(64, 64)\n",
        "        self.relu5 = nn.ReLU()\n",
        "\n",
        "        self.linear6 = nn.Linear(64, 32)\n",
        "        self.relu6 = nn.ReLU()\n",
        "\n",
        "        self.linear7 = nn.Linear(32, 16)\n",
        "        self.relu7 = nn.ReLU()\n",
        "\n",
        "        self.linear8 = nn.Linear(16, 8)\n",
        "        self.relu8 = nn.ReLU()\n",
        "\n",
        "        self.linear9 = nn.Linear(8, 6)\n",
        "        self.relu9 = nn.ReLU()\n",
        "\n",
        "        self.linear10 = nn.Linear(6, 4)\n",
        "        self.relu10 = nn.ReLU()\n",
        "\n",
        "        self.linear11 = nn.Linear(4, 1)\n",
        "        self.relu11 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        lin1_out = self.linear1(x)\n",
        "        relu_out1 = self.relu1(lin1_out)\n",
        "        relu_out2 = self.relu2(self.linear2(relu_out1))\n",
        "        relu_out3 = self.relu3(self.linear3(relu_out2))\n",
        "        relu_out4 = self.relu4(self.linear4(relu_out3))\n",
        "        relu_out5 = self.relu5(self.linear5(relu_out4))\n",
        "        relu_out6 = self.relu6(self.linear6(relu_out5))\n",
        "        relu_out7 = self.relu7(self.linear7(relu_out6))\n",
        "        relu_out8 = self.relu8(self.linear8(relu_out7))\n",
        "        relu_out9 = self.relu9(self.linear9(relu_out8))\n",
        "        relu_out10 = self.relu10(self.linear10(relu_out9))\n",
        "        relu_out11 = self.relu11(self.linear11(relu_out10))\n",
        "        return relu_out11\n",
        "\n",
        "\n",
        "from optimizer import Lookahead\n",
        "from radam import RAdam\n",
        "\n",
        "net = SimpleNNModel()\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "net.apply(init_weights)\n",
        "\n",
        "USE_PRETRAINED_MODEL = True\n",
        "n_batches = 32\n",
        "if USE_PRETRAINED_MODEL:\n",
        "    net.load_state_dict(torch.load(\"interpret_model.pt\"))\n",
        "    print(\"Model Loaded!\")\n",
        "else:\n",
        "    criterion = nn.MSELoss()\n",
        "    num_epochs = 400\n",
        "\n",
        "    # optimizer = torch.optim.AdamW(net.parameters(), lr=0.0003, weight_decay=0.01)\n",
        "    base_optim = RAdam(net.parameters(), lr=0.001)\n",
        "    optimizer = Lookahead(base_optim, k=6, alpha=0.5)\n",
        "    input_tensor = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
        "    label_tensor = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
        "    for epoch in range(num_epochs):\n",
        "        for i in range(n_batches):\n",
        "            local_X, local_y = (\n",
        "                input_tensor[i * n_batches : (i + 1) * n_batches,],\n",
        "                label_tensor[i * n_batches : (i + 1) * n_batches,],\n",
        "            )\n",
        "            output = net(input_tensor)\n",
        "            loss = criterion(output, label_tensor)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if epoch % 5 == 0:\n",
        "            print(\"Epoch {}/{} => Loss: {:.6f}\".format(epoch + 1, num_epochs, loss.item()))\n",
        "\n",
        "    torch.save(net.state_dict(), \"interpret_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omnq-6Ojk8Ie"
      },
      "outputs": [],
      "source": [
        "test_input_tensor = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
        "test_output = net(test_input_tensor).detach().numpy()\n",
        "print(\"Test MSE:\", np.sqrt((np.square(test_output - y_test)).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fywEXUPmk8Ie"
      },
      "outputs": [],
      "source": [
        "ig = IntegratedGradients(net)\n",
        "test_input_tensor = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
        "test_input_tensor.requires_grad_()\n",
        "attr, delta = ig.attribute(test_input_tensor, return_convergence_delta=True)\n",
        "attr = attr.detach().numpy()\n",
        "\n",
        "feature_names = list(column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOgYpRg1k8Ie"
      },
      "outputs": [],
      "source": [
        "def visualize_importances(\n",
        "    feature_names,\n",
        "    importances,\n",
        "    title=\"Intergrated Gradient Average Feature Importances\",\n",
        "    plot=True,\n",
        "    axis_title=\"Features\",\n",
        "):\n",
        "    print(title)\n",
        "    for i in range(len(feature_names)):\n",
        "        print(feature_names[i], \": \", \"%.3f\" % (importances[i]))\n",
        "    x_pos = np.arange(len(feature_names))\n",
        "    if plot:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.bar(x_pos, importances, align=\"center\")\n",
        "        plt.xticks(x_pos, feature_names, wrap=False)\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.xlabel(axis_title)\n",
        "        plt.title(title)\n",
        "\n",
        "\n",
        "visualize_importances(column_names, np.mean(attr, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLkuT4qck8Ie"
      },
      "outputs": [],
      "source": [
        "dnn_ig = np.abs(np.mean(attr, axis=0))\n",
        "dnn_ig = np.abs(dnn_ig)\n",
        "dnn_ig_min = np.abs(dnn_ig).min()\n",
        "dnn_ig_max = np.abs(dnn_ig).max()\n",
        "\n",
        "dnn_ig_scaled = (dnn_ig - dnn_ig_min) / (dnn_ig_max - dnn_ig_min)\n",
        "dnn_ig_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z612lpEUk8Ie"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(pos, (gb_sv_scaled + dnn_sv_scaled + rf_sv_scaled) / 3, align=\"center\", alpha=0.5, label=\"SHAP\")\n",
        "plt.bar(pos, ((gb_pi_scaled + rf_pi_scaled + dnn_pi_scaled) / 3), align=\"center\", alpha=0.5, label=\"PI\")\n",
        "plt.bar(pos, ((dnn_ig_scaled)), align=\"center\", alpha=0.5, label=\"IG\")\n",
        "plt.xticks(pos, column_names)\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"Ensemble average of SHAP vs Permutation importance vs IG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNTJwip4k8Ie"
      },
      "outputs": [],
      "source": [
        "sv = (gb_sv_scaled + dnn_sv_scaled + rf_sv_scaled) / 3\n",
        "pi = (gb_pi_scaled + rf_pi_scaled + dnn_pi_scaled) / 3\n",
        "ig = dnn_ig_scaled\n",
        "\n",
        "avg_all = (sv + pi + ig) / 3\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(pos, (avg_all), align=\"center\", alpha=0.7, label=\"all\")\n",
        "plt.xticks(pos, column_names)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.title(\"Ensemble feature importance of ten-fold cross validation experiment\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('Ensemble_feature_importance_TCV.png', dpi=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmKU6jsTk8Ie"
      },
      "outputs": [],
      "source": [
        "avg_all[-3:].sum()/avg_all.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywGROT5Zk8Ie"
      },
      "outputs": [],
      "source": [
        "avg_all[-3]/avg_all.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AezeTFxRk8Ie"
      },
      "source": [
        "# On unseen dataset. Leave one out for each condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTbu8zVUk8Ie"
      },
      "outputs": [],
      "source": [
        "# unseen_name = [\"45m\", \"45s\", \"abvsm\", \"hm\", \"hs\", \"vm\", \"vsm\", \"vss\"]\n",
        "# creep_rate = [0.00305, 0.00348, 0.0124, 0.00566, 0.00492, 0.00273, 0.00239, 0.00404]\n",
        "unseen_name = [\"45m\", \"45s\", \"hm\", \"hs\", \"vm\", \"vsm\", \"vss\"]\n",
        "creep_rate = [0.00305, 0.00348, 0.00566, 0.00492, 0.00273, 0.00239, 0.00404]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MJcI6aUk8Ie"
      },
      "outputs": [],
      "source": [
        "mean_rf_median_ae = []\n",
        "mean_rf_r2 = []\n",
        "mean_rf_mean_ae = []\n",
        "mean_rf_mape = []\n",
        "mean_rf_rmse = []\n",
        "\n",
        "std_rf_median_ae = []\n",
        "std_rf_r2 = []\n",
        "std_rf_mean_ae = []\n",
        "std_rf_mape = []\n",
        "std_rf_rmse = []\n",
        "\n",
        "mean_gb_median_ae = []\n",
        "mean_gb_r2 = []\n",
        "mean_gb_mean_ae = []\n",
        "mean_gb_mape = []\n",
        "mean_gb_rmse = []\n",
        "\n",
        "std_gb_median_ae = []\n",
        "std_gb_r2 = []\n",
        "std_gb_mean_ae = []\n",
        "std_gb_mape = []\n",
        "std_gb_rmse = []\n",
        "\n",
        "mean_dr_median_ae = []\n",
        "mean_dr_r2 = []\n",
        "mean_dr_mean_ae = []\n",
        "mean_dr_mape = []\n",
        "mean_dr_rmse = []\n",
        "\n",
        "std_dr_median_ae = []\n",
        "std_dr_r2 = []\n",
        "std_dr_mean_ae = []\n",
        "std_dr_mape = []\n",
        "std_dr_rmse = []\n",
        "\n",
        "all_rf_pi = []\n",
        "all_rf_sv = []\n",
        "all_gb_pi = []\n",
        "all_gb_sv = []\n",
        "\n",
        "plt.ioff()\n",
        "\n",
        "for i in range(len(unseen_name)):\n",
        "    rf_temp_median_ae = []\n",
        "    rf_temp_mean_ae = []\n",
        "    rf_temp_mape = []\n",
        "    rf_temp_rmse = []\n",
        "\n",
        "    gb_temp_median_ae = []\n",
        "    gb_temp_mean_ae = []\n",
        "    gb_temp_mape = []\n",
        "    gb_temp_rmse = []\n",
        "\n",
        "    dr_temp_median_ae = []\n",
        "    dr_temp_mean_ae = []\n",
        "    dr_temp_mape = []\n",
        "    dr_temp_rmse = []\n",
        "    \n",
        "    temp_rf_pi = []\n",
        "    temp_rf_sv = []\n",
        "    temp_gb_pi = []\n",
        "    temp_gb_sv = []\n",
        "    for j in range(10):\n",
        "        print(f\"Currently running unseen: {unseen_name[i]}...\")\n",
        "        df_creep_unseen = pd.read_csv(\n",
        "            f\"No anomaly data/complete_creep_no_anomaly_upsampled_unseen_{unseen_name[i]}.csv\"\n",
        "        )\n",
        "        df_creep_unseen.rename(columns={\"label\": \"num_holes\", \"laser_angle\": \"build_orientation\"}, inplace=True)\n",
        "        df_creep_unseen[\"laser_num\"] = df_creep_unseen[\"laser_num\"].astype(\"category\").cat.codes\n",
        "        df_creep_unseen[\"build_orientation\"] = df_creep_unseen[\"build_orientation\"].astype(\"category\").cat.codes\n",
        "        df_creep_unseen[\"scan_strategy\"] = df_creep_unseen[\"scan_strategy\"].astype(\"category\").cat.codes\n",
        "        df_creep_unseen.drop(\n",
        "            [\"euler_number\", \"extent\", \"conditions\", \"max_intensity\", \"mean_intensity\", \"min_intensity\"],\n",
        "            inplace=True,\n",
        "            axis=1,\n",
        "        )\n",
        "        df_creep_unseen_X = df_creep_unseen.drop([\"creep rate\"], axis=1)\n",
        "       \n",
        "\n",
        "        unseen_creep_data = df_creep_unseen.to_numpy()\n",
        "        unseen_creep_data_cont = unseen_creep_data[:, :-4]\n",
        "        unseen_creep_data_cat = unseen_creep_data[:, -4:]\n",
        "\n",
        "        min_max_scaler = preprocessing.MinMaxScaler()\n",
        "        unseen_creep_data_cont_scaled = min_max_scaler.fit_transform(unseen_creep_data_cont)\n",
        "        unseen_creep_data = np.hstack((unseen_creep_data_cont_scaled, unseen_creep_data_cat))\n",
        "        unseen_creep_data_X = unseen_creep_data[:, :-1]\n",
        "        unseen_creep_data_y = unseen_creep_data[:, -1]\n",
        "        unseen_creep_data_y = unseen_creep_data_y * 10000\n",
        "\n",
        "        train_test_ratio = (\n",
        "            df_creep_unseen[df_creep_unseen[\"creep rate\"] == creep_rate[i]].shape[0] / df_creep_unseen.shape[0]\n",
        "        )\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            unseen_creep_data_X, unseen_creep_data_y, test_size=train_test_ratio, shuffle=False\n",
        "        )\n",
        "\n",
        "        #####################################################################################################\n",
        "        # Random Forest\n",
        "        print(f\"Currently running unseen: {unseen_name[i]} on Random Forest...\")\n",
        "        rf_regr = RandomForestRegressor(max_depth=5, n_estimators=500)\n",
        "        rf_regr.fit(X_train, y_train)\n",
        "        rf_predicted = rf_regr.predict(X_test)\n",
        "\n",
        "        rf_temp_median_ae.append(median_absolute_error(y_test, rf_predicted))\n",
        "        # rf_r2.append(r2_score(y_test, rf_predicted))\n",
        "        rf_temp_mean_ae.append(mean_absolute_error(y_test, rf_predicted))\n",
        "        rf_temp_mape.append(mean_absolute_percentage_error(y_test, rf_predicted))\n",
        "        rf_temp_rmse.append(mean_squared_error(y_test, rf_predicted, squared=False))\n",
        "\n",
        "        ## Random Forest Creep Rate prediction\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.plot(y_test, label=\"True\")\n",
        "        plt.plot(rf_predicted, label=\"Predicted\")\n",
        "        plt.legend(loc=\"best\")\n",
        "        plt.title(f\"Random Forest prediction of creep rate with unseen:{unseen_name[i]}\")\n",
        "        plt.savefig(f\"RF_prediction_unseen_{unseen_name[i]}\")\n",
        "        plt.close()\n",
        "        ## Random Forest feature importance using Mean Decrease Impurity\n",
        "\n",
        "        rf_feature_imp = rf_regr.feature_importances_\n",
        "        pos = np.arange(column_names.size)\n",
        "\n",
        "        combined_rf_imp_name = np.stack((rf_feature_imp, column_names), axis=-1)\n",
        "        sorted_combined_rf_imp_name = combined_rf_imp_name[combined_rf_imp_name[:, 0].argsort()]\n",
        "\n",
        "        sorted_rf_feature_imp = sorted_combined_rf_imp_name[:, 0]\n",
        "        sorted_column_names = sorted_combined_rf_imp_name[:, 1]\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.bar(pos, sorted_rf_feature_imp, align=\"center\", alpha=0.5)\n",
        "        plt.xticks(pos, sorted_column_names)\n",
        "        plt.title(\n",
        "            f\"Random Forest feature importance using Mean Decrease in Impurity Importances (test set) unseen:{unseen_name[i]}\"\n",
        "        )\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.savefig(f\"RF_MDI_Importances_unseen_{unseen_name[i]}\")\n",
        "        plt.close()\n",
        "        ## Random Forest feature importance using Permutation importance\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        pi_result_rf = permutation_importance(rf_regr, X_test, y_test, n_repeats=10, random_state=42)\n",
        "        pi_result_rf_relu = np.maximum(pi_result_rf.importances_mean, 0)\n",
        "        rf_pi = np.abs(pi_result_rf_relu)\n",
        "        temp_rf_pi.append(normalise(rf_pi).tolist())\n",
        "        \n",
        "        sorted_idx = pi_result_rf.importances_mean.argsort()\n",
        "\n",
        "        plt.bar(pos, pi_result_rf.importances_mean[sorted_idx].T, align=\"center\", alpha=0.5)\n",
        "        plt.xticks(pos, column_names[sorted_idx])\n",
        "        plt.title(f\"Random Forest Permutation Importances (test set) unseen:{unseen_name[i]}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.savefig(f\"RF_PI_unseen_{unseen_name[i]}\")\n",
        "        plt.close()\n",
        "        sorted_idx = pi_result_rf.importances_mean.argsort()\n",
        "        \n",
        "        \n",
        "        # RF SV\n",
        "        explainer_rf = shap.TreeExplainer(rf_regr)\n",
        "        shap_values_rf = explainer_rf.shap_values(df_creep_unseen_X)\n",
        "        mean_rf_sv = np.maximum(np.abs(shap_values_rf.tolist()).mean(0),0)\n",
        "        temp_rf_sv.append(mean_rf_sv)\n",
        "\n",
        "        \n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.boxplot(pi_result_rf.importances[sorted_idx].T, vert=True, labels=column_names[sorted_idx])\n",
        "        plt.title(f\"Random Forest Permutation Importances (test set) with mean and std unseen:{unseen_name[i]}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.savefig(f\"RF_PI_STATS_unseen_{unseen_name[i]}\")\n",
        "        plt.close()\n",
        "\n",
        "        #####################################################################################################\n",
        "        # Gradient Boosting\n",
        "\n",
        "        print(f\"Currently running unseen: {unseen_name[i]} on Gradient Boosting...\")\n",
        "        gb_regr = GradientBoostingRegressor(max_depth=5, n_estimators=500)\n",
        "        gb_regr.fit(X_train, y_train)\n",
        "        gb_predicted = gb_regr.predict(X_test)\n",
        "\n",
        "        gb_temp_median_ae.append(median_absolute_error(y_test, gb_predicted))\n",
        "        # gb_r2.append(r2_score(y_test, gb_predicted))\n",
        "        gb_temp_mean_ae.append(mean_absolute_error(y_test, gb_predicted))\n",
        "        gb_temp_mape.append(mean_absolute_percentage_error(y_test, gb_predicted))\n",
        "        gb_temp_rmse.append(mean_squared_error(y_test, gb_predicted, squared=False))\n",
        "\n",
        "        ## Gradient Boosting Creep Rate prediction\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.plot(y_test, label=\"True\")\n",
        "        plt.plot(gb_predicted, label=\"Predicted\")\n",
        "        plt.legend(loc=\"best\")\n",
        "        plt.title(f\"Gradient Boosting prediction of creep rate with unseen:{unseen_name[i]}\")\n",
        "        plt.savefig(f\"GB_prediction_unseen_{unseen_name[i]}\")\n",
        "        plt.close()\n",
        "        ## Gradient Boosting feature importance using Mean Decrease Impurity\n",
        "\n",
        "        gb_feature_imp = gb_regr.feature_importances_\n",
        "        pos = np.arange(column_names.size)\n",
        "\n",
        "        combined_gb_imp_name = np.stack((gb_feature_imp, column_names), axis=-1)\n",
        "        sorted_combined_gb_imp_name = combined_gb_imp_name[combined_gb_imp_name[:, 0].argsort()]\n",
        "\n",
        "        sorted_gb_feature_imp = sorted_combined_gb_imp_name[:, 0]\n",
        "        sorted_column_names = sorted_combined_gb_imp_name[:, 1]\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.bar(pos, sorted_gb_feature_imp, align=\"center\", alpha=0.5)\n",
        "        plt.xticks(pos, sorted_column_names)\n",
        "        plt.title(\n",
        "            f\"Gradient Boosting feature importance using Mean Decrease in Impurity Importances (test set) unseen:{unseen_name[i]}\"\n",
        "        )\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.savefig(f\"GB_MDI_Importances_unseen_{unseen_name[i]}\")\n",
        "        plt.close()\n",
        "        ## Gradient Boosting feature importance using Permutation importance\n",
        "\n",
        "        pi_result_gb = permutation_importance(gb_regr, X_test, y_test, n_repeats=10, random_state=42)\n",
        "        pi_result_gb_relu = np.maximum(pi_result_gb.importances_mean, 0)\n",
        "        gb_pi = np.abs(pi_result_gb_relu)\n",
        "        temp_gb_pi.append(normalise(gb_pi).tolist())\n",
        "        \n",
        "        sorted_idx = pi_result_gb.importances_mean.argsort()\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.bar(pos, pi_result_gb.importances_mean[sorted_idx].T, align=\"center\", alpha=0.5)\n",
        "        plt.xticks(pos, column_names[sorted_idx])\n",
        "        plt.title(f\"Gradient Boosting Permutation Importances (test set) unseen:{unseen_name[i]}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.savefig(f\"GB_PI_unseen_{unseen_name[i]}\")\n",
        "        plt.close()\n",
        "        sorted_idx = pi_result_gb.importances_mean.argsort()\n",
        "        \n",
        "        ## GB SV\n",
        "        \n",
        "        explainer_gb = shap.TreeExplainer(gb_regr)\n",
        "        shap_values_gb = explainer_gb.shap_values(df_creep_unseen_X)\n",
        "        mean_gb_sv = np.maximum(np.abs(shap_values_gb).mean(0),0)\n",
        "        temp_gb_sv.append(mean_gb_sv.tolist())\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.boxplot(pi_result_gb.importances[sorted_idx].T, vert=True, labels=column_names[sorted_idx])\n",
        "        plt.title(f\"Gradient Boosting Permutation Importances (test set) with mean and std unseen:{unseen_name[i]}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.savefig(f\"GB_PI_STATS_unseen_{unseen_name[i]}\")\n",
        "        plt.close()\n",
        "\n",
        "        #####################################################################################################\n",
        "        # Dummy regressor\n",
        "\n",
        "        print(f\"Currently running unseen: {unseen_name[i]} on Dummy regressor...\")\n",
        "        dr_regr = DummyRegressor(strategy=\"mean\")\n",
        "        dr_regr.fit(X_train, y_train)\n",
        "        dr_predicted = dr_regr.predict(X_test)\n",
        "\n",
        "        dr_temp_median_ae.append(median_absolute_error(y_test, dr_predicted))\n",
        "        # dr_r2.append(r2_score(y_test, dr_predicted))\n",
        "        dr_temp_mean_ae.append(mean_absolute_error(y_test, dr_predicted))\n",
        "        dr_temp_mape.append(mean_absolute_percentage_error(y_test, dr_predicted))\n",
        "        dr_temp_rmse.append(mean_squared_error(y_test, dr_predicted, squared=False))\n",
        "\n",
        "        ## Dummy regressor Creep Rate prediction\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.plot(y_test, label=\"True\")\n",
        "        plt.plot(dr_predicted, label=\"Predicted\")\n",
        "        plt.legend(loc=\"best\")\n",
        "        plt.title(f\"Dummy regressor prediction of creep rate with unseen:{unseen_name[i]}\")\n",
        "        plt.savefig(f\"DR_prediction_unseen_{unseen_name[i]}\")\n",
        "        plt.close()\n",
        "    \n",
        "    all_rf_pi.append(np.array(temp_rf_pi).mean(0))\n",
        "    all_gb_pi.append(np.array(temp_gb_pi).mean(0))\n",
        "    all_rf_sv.append(np.array(temp_rf_sv).mean(0))\n",
        "    all_gb_sv.append(np.array(temp_gb_sv).mean(0))\n",
        "    \n",
        "    mean_rf_median_ae.append(mean(rf_temp_median_ae))\n",
        "    mean_rf_mean_ae.append(mean(rf_temp_mean_ae))\n",
        "    mean_rf_mape.append(mean(rf_temp_mape))\n",
        "    mean_rf_rmse.append(mean(rf_temp_rmse))\n",
        "\n",
        "    std_rf_median_ae.append(stdev(rf_temp_median_ae))\n",
        "    std_rf_mean_ae.append(stdev(rf_temp_mean_ae))\n",
        "    std_rf_mape.append(stdev(rf_temp_mape))\n",
        "    std_rf_rmse.append(stdev(rf_temp_rmse))\n",
        "\n",
        "    mean_gb_median_ae.append(mean(gb_temp_median_ae))\n",
        "    mean_gb_mean_ae.append(mean(gb_temp_mean_ae))\n",
        "    mean_gb_mape.append(mean(gb_temp_mape))\n",
        "    mean_gb_rmse.append(mean(gb_temp_rmse))\n",
        "\n",
        "    std_gb_median_ae.append(stdev(gb_temp_median_ae))\n",
        "    std_gb_mean_ae.append(stdev(gb_temp_mean_ae))\n",
        "    std_gb_mape.append(stdev(gb_temp_mape))\n",
        "    std_gb_rmse.append(stdev(gb_temp_rmse))\n",
        "\n",
        "    mean_dr_median_ae.append(mean(dr_temp_median_ae))\n",
        "    mean_dr_mean_ae.append(mean(dr_temp_mean_ae))\n",
        "    mean_dr_mape.append(mean(dr_temp_mape))\n",
        "    mean_dr_rmse.append(mean(dr_temp_rmse))\n",
        "\n",
        "    std_dr_median_ae.append(stdev(dr_temp_median_ae))\n",
        "    std_dr_mean_ae.append(stdev(dr_temp_mean_ae))\n",
        "    std_dr_mape.append(stdev(dr_temp_mape))\n",
        "    std_dr_rmse.append(stdev(dr_temp_rmse))\n",
        "\n",
        "print(\"Finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "LO55vuRmk8If"
      },
      "outputs": [],
      "source": [
        "# plt.bar(pos,np.array(all_rf_sv[0]))\n",
        "# plt.xticks(pos, column_names)\n",
        "# plt.xticks(rotation=90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY_s4wTNk8If"
      },
      "outputs": [],
      "source": [
        "def helper_rank(rank_function, data, methods_columns_name):\n",
        "\n",
        "    # calculate rank correlation and p-value. null hypothesis is that two sets of data are uncorrelated\n",
        "    all_corr = np.zeros((data.shape[1], data.shape[1]))\n",
        "    all_p_value = np.zeros((data.shape[1], data.shape[1]))\n",
        "    for i in range(data.shape[1]):\n",
        "        for j in range(data.shape[1]):\n",
        "            corr, p_value = rank_function(data[:, i], data[:, j])\n",
        "            all_corr[i, j] = corr\n",
        "            all_p_value[i, j] = p_value\n",
        "\n",
        "    # store correlation and p-value in dataframe\n",
        "    df_corr = pd.DataFrame(all_corr, columns=methods_columns_name, index=methods_columns_name)\n",
        "    df_p_value = pd.DataFrame(all_p_value, columns=methods_columns_name, index=methods_columns_name)\n",
        "\n",
        "    # generate truth table for p-value. If p value < 0.05(significant) value is True/1.0\n",
        "    df_p_value_significance = df_p_value < 0.05\n",
        "\n",
        "    # remove diagonal significance as it is measured against itself\n",
        "    for i in range(df_p_value.shape[0]):\n",
        "        df_p_value_significance.iloc[i, i] = None\n",
        "\n",
        "    # decide if feature importance method should be kept\n",
        "    # if majority are correlated and statistically significant then method are kept\n",
        "    keep_FI_method = []\n",
        "    keep_or_not = True\n",
        "\n",
        "    # zeros placeholder same number of rows/feature\n",
        "    all_stacked_corr = np.zeros((data.shape[0], 1))\n",
        "\n",
        "    # loop to include only majority voted method. Majority voted in this case is equivalent\n",
        "    # to all method deemed correlated by statistical significance\n",
        "    for i in range(df_p_value_significance.shape[1]):\n",
        "        keep_or_not = Counter(df_p_value_significance.iloc[:, i]).most_common()[0][0]\n",
        "        keep_FI_method.append(keep_or_not)\n",
        "        if keep_or_not:\n",
        "            all_stacked_corr = np.hstack((all_stacked_corr, np.reshape(data[:, i], (-1, 1))))\n",
        "\n",
        "    # remove the zeros placeholder for shape\n",
        "    all_stacked_corr = all_stacked_corr[:, 1:]\n",
        "\n",
        "    # calculate mean for each feature\n",
        "    total_scaled_corr = np.mean(all_stacked_corr, axis=1)\n",
        "    return total_scaled_corr, df_p_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsrG8b8mk8If"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "all_stacked = np.hstack(\n",
        "    (\n",
        "        np.reshape(normalise(all_gb_sv[0]), (normalise(all_gb_sv[0]).shape[0],1)),\n",
        "        np.reshape(normalise(all_gb_sv[1]), (normalise(all_gb_sv[1]).shape[0],1)),\n",
        "        np.reshape(normalise(all_gb_sv[3]), (normalise(all_gb_sv[3]).shape[0],1)),\n",
        "        np.reshape(normalise(all_gb_sv[4]), (normalise(all_gb_sv[4]).shape[0],1)),\n",
        "        np.reshape(normalise(all_gb_sv[6]), (normalise(all_gb_sv[6]).shape[0],1)),\n",
        "        np.reshape(normalise(all_rf_sv[0]), (normalise(all_rf_sv[0]).shape[0],1)),\n",
        "        np.reshape(normalise(all_rf_sv[1]), (normalise(all_rf_sv[1]).shape[0],1)),\n",
        "        np.reshape(normalise(all_rf_sv[3]), (normalise(all_rf_sv[3]).shape[0],1)),\n",
        "        np.reshape(normalise(all_rf_sv[4]), (normalise(all_rf_sv[4]).shape[0],1)),\n",
        "        np.reshape(normalise(all_rf_sv[6]), (normalise(all_rf_sv[6]).shape[0],1)),\n",
        "\n",
        "    )\n",
        ")\n",
        "\n",
        "columns_name = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"]\n",
        "total_scaled_kendall_tau, kendall_tau_p_value = helper_rank(stats.kendalltau, all_stacked, columns_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU-q9EBwk8If"
      },
      "outputs": [],
      "source": [
        "#plt.bar(pos,total_scaled_kendall_tau)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(pos, total_scaled_kendall_tau, align=\"center\", alpha=0.7)\n",
        "plt.xticks(pos, column_names)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.title(\"Ensemble feature importance of leave-one-condition-out experiment\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('Ensemble_feature_importance_LOCO.png', dpi=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDSKsyOmk8If"
      },
      "outputs": [],
      "source": [
        "total_scaled_kendall_tau[-1]/total_scaled_kendall_tau.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikiZBiLFk8If"
      },
      "outputs": [],
      "source": [
        "normalise(all_gb_sv[0]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qx1Txljk8If"
      },
      "outputs": [],
      "source": [
        "indices = [0, 1, 3, 4, 6]\n",
        "np.take(all_gb_sv, indices, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPt7Lmx0k8If"
      },
      "outputs": [],
      "source": [
        "mean_dnn_median_ae = []\n",
        "mean_dnn_mean_ae = []\n",
        "mean_dnn_mape = []\n",
        "mean_dnn_rmse = []\n",
        "\n",
        "std_dnn_median_ae = []\n",
        "std_dnn_mean_ae = []\n",
        "std_dnn_mape = []\n",
        "std_dnn_rmse = []\n",
        "\n",
        "\n",
        "total_run = 0\n",
        "\n",
        "for i in range(len(unseen_name)):\n",
        "    counter = 0\n",
        "    temp_dnn_median_ae = []\n",
        "    temp_dnn_mean_ae = []\n",
        "    temp_dnn_mape = []\n",
        "    temp_dnn_rmse = []\n",
        "    print(f\"Currently running unseen: {unseen_name[i]} on DNN regressor...\")\n",
        "    while counter < 10:\n",
        "        df_creep_unseen = pd.read_csv(\n",
        "            f\"No anomaly data/complete_creep_no_anomaly_upsampled_unseen_{unseen_name[i]}.csv\"\n",
        "        )\n",
        "        df_creep_unseen.rename(columns={\"label\": \"num_holes\", \"laser_angle\": \"build_orientation\"}, inplace=True)\n",
        "        df_creep_unseen[\"laser_num\"] = df_creep_unseen[\"laser_num\"].astype(\"category\").cat.codes\n",
        "        df_creep_unseen[\"build_orientation\"] = df_creep_unseen[\"build_orientation\"].astype(\"category\").cat.codes\n",
        "        df_creep_unseen[\"scan_strategy\"] = df_creep_unseen[\"scan_strategy\"].astype(\"category\").cat.codes\n",
        "        df_creep_unseen.drop(\n",
        "            [\"euler_number\", \"extent\", \"conditions\", \"max_intensity\", \"mean_intensity\", \"min_intensity\"],\n",
        "            inplace=True,\n",
        "            axis=1,\n",
        "        )\n",
        "        \n",
        "\n",
        "        unseen_creep_data = df_creep_unseen.to_numpy()\n",
        "        unseen_creep_data_cont = unseen_creep_data[:, :-4]\n",
        "        unseen_creep_data_cat = unseen_creep_data[:, -4:]\n",
        "\n",
        "        min_max_scaler = preprocessing.MinMaxScaler()\n",
        "        unseen_creep_data_cont_scaled = min_max_scaler.fit_transform(unseen_creep_data_cont)\n",
        "        unseen_creep_data = np.hstack((unseen_creep_data_cont_scaled, unseen_creep_data_cat))\n",
        "        unseen_creep_data_X = unseen_creep_data[:, :-1]\n",
        "        unseen_creep_data_y = unseen_creep_data[:, -1]\n",
        "        unseen_creep_data_y = unseen_creep_data_y * 10000\n",
        "\n",
        "        train_test_ratio = (\n",
        "            df_creep_unseen[df_creep_unseen[\"creep rate\"] == creep_rate[i]].shape[0] / df_creep_unseen.shape[0]\n",
        "        )\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            unseen_creep_data_X, unseen_creep_data_y, test_size=train_test_ratio, shuffle=False\n",
        "        )\n",
        "        #####################################################################################################\n",
        "        ## DNN regressor\n",
        "\n",
        "        def base_model():\n",
        "            model = tf.keras.Sequential(\n",
        "                [\n",
        "                    layers.Dense(\n",
        "                        128,\n",
        "                        activation=\"relu\",\n",
        "                        input_shape=(creep_data_X.shape[1],),\n",
        "                        bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "                    ),\n",
        "                    layers.Dense(256, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "                    layers.Dense(128, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "                    layers.Dense(64, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "                    layers.Dense(64, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "                    layers.Dense(32, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "                    layers.Dense(16, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "                    layers.Dense(8, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "                    layers.Dense(6, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "                    layers.Dense(4, activation=\"relu\", bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "                    layers.Dense(1, activation=\"relu\"),\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            radam = tfa.optimizers.RectifiedAdam()\n",
        "            ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n",
        "            # tf.keras.optimizers.Adam(1e-4)\n",
        "            # Configure a model for mean-squared error regression.\n",
        "            model.compile(optimizer=ranger, loss=\"mse\", metrics=[\"mae\"])  # mean squared error  # mean absolute error\n",
        "            return model\n",
        "\n",
        "        # reshape for deep model\n",
        "        y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
        "        y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
        "\n",
        "        PATIENCE = 100\n",
        "        callback = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_loss\",\n",
        "            min_delta=0,\n",
        "            patience=PATIENCE,\n",
        "            verbose=0,\n",
        "            mode=\"auto\",\n",
        "            baseline=None,\n",
        "            restore_best_weights=True,\n",
        "        )\n",
        "\n",
        "        dnn_model = KerasRegressor(\n",
        "            build_fn=base_model, epochs=1000, batch_size=32, verbose=0, validation_split=0.1, callbacks=[callback]\n",
        "        )\n",
        "\n",
        "        history = dnn_model.fit(X_train, y_train)\n",
        "        total_run += 1\n",
        "        if len(history.history[\"val_loss\"]) > PATIENCE + 105:\n",
        "            dnn_predicted = dnn_model.predict(X_test)\n",
        "\n",
        "            dnn_predicted = dnn_predicted.reshape(dnn_predicted.shape[0], 1)\n",
        "\n",
        "            temp_dnn_median_ae.append(median_absolute_error(y_test, dnn_predicted))\n",
        "            temp_dnn_mean_ae.append(mean_absolute_error(y_test, dnn_predicted))\n",
        "            temp_dnn_mape.append(mean_absolute_percentage_error(y_test, dnn_predicted))\n",
        "            temp_dnn_rmse.append(mean_squared_error(y_test, dnn_predicted, squared=False))\n",
        "            counter += 1\n",
        "\n",
        "            perm = PermutationImportance(dnn_model, scoring=mae_scorer, random_state=1).fit(X_train, y_train)\n",
        "            dnn_pi_df = eli5.explain_weights_df(perm, feature_names=column_names.tolist())\n",
        "\n",
        "    #     print(f'{unseen_name[i]} median_ae: {temp_dnn_median_ae}')\n",
        "    #     print(f'{unseen_name[i]} mean_ae: {temp_dnn_mean_ae}')\n",
        "    #     print(f'{unseen_name[i]} mape: {temp_dnn_mape}')\n",
        "    #     print(f'{unseen_name[i]} rmse: {temp_dnn_rmse}')\n",
        "\n",
        "    mean_dnn_median_ae.append((mean(temp_dnn_median_ae)))\n",
        "    mean_dnn_mean_ae.append((mean(temp_dnn_mean_ae)))\n",
        "    mean_dnn_mape.append((mean(temp_dnn_mape)))\n",
        "    mean_dnn_rmse.append((mean(temp_dnn_rmse)))\n",
        "\n",
        "    std_dnn_median_ae.append((stdev(temp_dnn_median_ae)))\n",
        "    std_dnn_mean_ae.append((stdev(temp_dnn_median_ae)))\n",
        "    std_dnn_mape.append((stdev(temp_dnn_mape)))\n",
        "    std_dnn_rmse.append((stdev(temp_dnn_rmse)))\n",
        "\n",
        "    ## DNN PI\n",
        "    pos = np.arange(column_names.size)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.bar(pos, dnn_pi_df.weight.values[::-1], align=\"center\", alpha=0.5)\n",
        "    plt.xticks(pos, dnn_pi_df.feature.values[::-1])\n",
        "    plt.title(\"DNN Permutation Importances (test set)\")\n",
        "    plt.savefig(f\"DNN_PI_unseen_{unseen_name[i]}\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.close()\n",
        "    ## DNN ERROR\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.plot(y_test, label=\"True\")\n",
        "    plt.plot(dnn_predicted, label=\"Predicted\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.title(f\"Deep Neural Network prediction of creep rate with unseen:{unseen_name[i]}\")\n",
        "    plt.savefig(f\"DNN_prediction_unseen_{unseen_name[i]}\")\n",
        "    plt.close()\n",
        "\n",
        "print(f\"Total run: {total_run}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFJnNzsNk8Ig"
      },
      "outputs": [],
      "source": [
        "mean_dnn_median_ae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPFTToYik8Ig"
      },
      "outputs": [],
      "source": [
        "# mean_dnn_median_ae = []\n",
        "# mean_dnn_mean_ae = []\n",
        "# mean_dnn_mape = []\n",
        "# mean_dnn_rmse = []\n",
        "\n",
        "# std_dnn_median_ae = []\n",
        "# std_dnn_mean_ae = []\n",
        "# std_dnn_mape = []\n",
        "# std_dnn_rmse = []\n",
        "\n",
        "# for i in range(8):\n",
        "#     mean_dnn_median_ae.append(mean(temp_dnn_median_ae[(i*10):(i*10)+10]))\n",
        "#     mean_dnn_mean_ae.append(mean(temp_dnn_mean_ae[(i*10):(i*10)+10]))\n",
        "#     mean_dnn_mape.append(mean(temp_dnn_mape[(i*10):(i*10)+10]))\n",
        "#     mean_dnn_rmse.append(mean(temp_dnn_rmse[(i*10):(i*10)+10]))\n",
        "\n",
        "#     std_dnn_median_ae.append(stdev(temp_dnn_median_ae[(i*10):(i*10)+10]))\n",
        "#     std_dnn_mean_ae.append(stdev(temp_dnn_mean_ae[(i*10):(i*10)+10]))\n",
        "#     std_dnn_mape.append(stdev(temp_dnn_mape[(i*10):(i*10)+10]))\n",
        "#     std_dnn_rmse.append(stdev(temp_dnn_rmse[(i*10):(i*10)+10]))\n",
        "# #len(temp_dnn_median_ae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQVqISSlk8Ig"
      },
      "outputs": [],
      "source": [
        "std_dnn_median_ae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHtl-wQGk8Ig"
      },
      "outputs": [],
      "source": [
        "def combine_mean_std(mean_array, std_array):\n",
        "    combined = []\n",
        "    for i in range(len(mean_array)):\n",
        "        combined.append(f\"{mean_array[i]:.4f}\\u00B1{std_array[i]:.4f}\")\n",
        "\n",
        "    return combined\n",
        "\n",
        "\n",
        "unseen_name = [\"45m\", \"45s\", \"hm\", \"hs\", \"vm\", \"vsm\", \"vss\"]\n",
        "\n",
        "multi_index_array = [\n",
        "    [\n",
        "        \"Random Forest\",\n",
        "        \"Random Forest\",\n",
        "        \"Random Forest\",\n",
        "        \"Random Forest\",\n",
        "        \"Gradient Boosting\",\n",
        "        \"Gradient Boosting\",\n",
        "        \"Gradient Boosting\",\n",
        "        \"Gradient Boosting\",\n",
        "        \"Deep Neural Network\",\n",
        "        \"Deep Neural Network\",\n",
        "        \"Deep Neural Network\",\n",
        "        \"Deep Neural Network\",\n",
        "        \"Dummy Regressor\",\n",
        "        \"Dummy Regressor\",\n",
        "        \"Dummy Regressor\",\n",
        "        \"Dummy Regressor\",\n",
        "    ],\n",
        "    [\n",
        "        \"MedAE\",\n",
        "        \"MAE\",\n",
        "        \"MAPE\",\n",
        "        \"RMSE\",\n",
        "        \"MedAE\",\n",
        "        \"MAE\",\n",
        "        \"MAPE\",\n",
        "        \"RMSE\",\n",
        "        \"MedAE\",\n",
        "        \"MAE\",\n",
        "        \"MAPE\",\n",
        "        \"RMSE\",\n",
        "        \"MedAE\",\n",
        "        \"MAE\",\n",
        "        \"MAPE\",\n",
        "        \"RMSE\",\n",
        "    ],\n",
        "]\n",
        "\n",
        "multi_index_tuple = list(zip(*multi_index_array))\n",
        "\n",
        "index = pd.MultiIndex.from_tuples(multi_index_tuple, names=[\"Models\", \"Evaluation Metrics\"])\n",
        "\n",
        "result_df = pd.DataFrame(\n",
        "    np.array(\n",
        "        [\n",
        "            combine_mean_std(mean_rf_median_ae, std_rf_median_ae),\n",
        "            combine_mean_std(mean_rf_mean_ae, std_rf_mean_ae),\n",
        "            combine_mean_std(mean_rf_mape, std_rf_mape),\n",
        "            combine_mean_std(mean_rf_rmse, std_rf_rmse),\n",
        "            combine_mean_std(mean_gb_median_ae, std_gb_median_ae),\n",
        "            combine_mean_std(mean_gb_mean_ae, std_gb_mean_ae),\n",
        "            combine_mean_std(mean_gb_mape, std_gb_mape),\n",
        "            combine_mean_std(mean_gb_rmse, std_gb_rmse),\n",
        "            combine_mean_std(mean_dnn_median_ae, std_dnn_median_ae),\n",
        "            combine_mean_std(mean_dnn_mean_ae, std_dnn_mean_ae),\n",
        "            combine_mean_std(mean_dnn_mape, std_dnn_mape),\n",
        "            combine_mean_std(mean_dnn_rmse, std_dnn_rmse),\n",
        "            combine_mean_std(mean_dr_median_ae, std_dr_median_ae),\n",
        "            combine_mean_std(mean_dr_mean_ae, std_dr_mean_ae),\n",
        "            combine_mean_std(mean_dr_mape, std_dr_mape),\n",
        "            combine_mean_std(mean_dr_rmse, std_dr_rmse),\n",
        "        ]\n",
        "    ),\n",
        "    columns=unseen_name,\n",
        "    index=index,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcGZO_5ck8Ig"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHk_LYubk8Ig"
      },
      "outputs": [],
      "source": [
        "rf_rmse_45s = np.random.normal(4.2077, 0.0357, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYPvj0Jik8Ig"
      },
      "outputs": [],
      "source": [
        "gb_rmse_45s = np.random.normal(4.3550, 0.0114, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y1iVb8Ck8Ig"
      },
      "outputs": [],
      "source": [
        "dnn_rmse_45s = np.random.normal(8.1479, 8.0545, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r5mMylQk8Ig"
      },
      "outputs": [],
      "source": [
        "dm_rmse_45s = np.random.normal(17.2612, 0, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phugXKYAk8Ig"
      },
      "outputs": [],
      "source": [
        "# dm vs rf\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egtkAbhpk8Ig"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"Are the result unseen 45s between dummy regressor and random forest significant? {is_it_sig(rf_rmse_45s,dm_rmse_45s)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Are the result unseen 45s between dummy regressor and gradient boosting significant? {is_it_sig(gb_rmse_45s,dm_rmse_45s)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Are the result unseen 45s between dummy regressor and deep neural network significant? {is_it_sig(rf_rmse_45s,dnn_rmse_45s)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Are the result unseen 45s between gradient boosting and random forest significant? {is_it_sig(rf_rmse_45s,gb_rmse_45s)}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGhOih_Rk8Ig"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCQGZlO_k8Ig"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}